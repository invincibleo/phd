<!DOCTYPE HTML>
<html>
	<head>
		<title>Supervised Contrastive Embeddings for Binaural Source Localization</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
	</head>
	<body class="is-preload">
		<!-- One -->
		<section id="one">
			<h1>Coming soon</h1>
			<h3>Duowei Tang, Peter Kuppens, Luc Geurts, Toon van Waterschoot</h3>
			<h6>Submitted to EUSIPCO 2020, Amsterdam, Netherlands</h6>
			<ul class="actions">
				<li><a href="file:///Users/invincibleo/Downloads/20-47.pdf" target="_blank" class="button icon solid small fa-download">PDF</a></li>
			</ul>
			<br />
			<h2>Abstract</h2>
			<p>In state-of-the-art end-to-end Speech Emotion Recognition (SER) systems, Convolutional Neural Network (CNN) layers are typically used to extract affective features while Long Short-Term Memory (LSTM) layers model longterm temporal dependencies. However, these systems suffer from several problems: 1) the model largely ignores temporal structure in speech due to the limited receptive field of the CNN layers, 2) the model inherits the drawbacks of Recurrent Neural Network (RNN)s, e.g. the gradient exploding/vanishing problem, the polynomial growth of computation time with the input sequence length and the lack of parallelizability. In this work, we propose a novel end-to-end SER structure that does not contain any recurrent or fully connected layers. By levering the power of the dilated causal convolution, the receptive field of the proposed model largely increases with reasonably low computational cost. By also using context stacking, the proposed model is capable of exploiting long-term temporal dependencies and can be an alternative to RNN. Experiments on the RECOLA database publicly available partition show improved results compare to a state-of-the-art system. We also verify that both the proposed model and the state-of-the-art model learned from short sequences (i.e. 20 s) can make accurate predictions for very long sequences (e.g.> 75 s).</p>
			<ul class="actions">
				<li><a class="button" onclick="goBack()">Back</a></li>
			</ul>
		</section>

		<script>
		function goBack() {
		  window.history.back()
		}
		</script>
	</body>
</html>